{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Griffin Ablation Study for MNIST (12 Configurations)\n\nimport os\nimport time\nimport random\nimport contextlib\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Optimizer\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, random_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nimport json\nfrom dataclasses import dataclass\nfrom typing import Tuple, Dict, List, Any\n\n# ---------- Global Performance Knobs ----------\nUSE_AMP = torch.cuda.is_available()\nCHANNELS_LAST = False  # MNIST is grayscale, so channels_last not beneficial\nAUTO_BENCHMARK = True\nMATMUL_PRECISION = \"high\"\n\ndef set_seed(seed=42):\n    \"\"\"Sets the seed for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = False\n        torch.backends.cudnn.benchmark = AUTO_BENCHMARK\n        try:\n            torch.set_float32_matmul_precision(MATMUL_PRECISION)\n        except AttributeError:\n            print(\"Warning: torch.set_float32_matmul_precision is not available.\")\n\nset_seed(42)\n\n# ---------------------------\n# Griffin Optimizer Implementation\n# ---------------------------\nclass Griffin(Optimizer):\n    def __init__(self, params, lr=5e-4, betas=(0.95, 0.99), weight_decay=1e-4,\n                 eps=1e-8, beta_sigma=0.9, schedule_decay=5e-3, warmup_steps=100):\n        if not 0.0 <= lr:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if not 0.0 <= eps:\n            raise ValueError(f\"Invalid epsilon value: {eps}\")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f\"Invalid beta-1 parameter: {betas[0]}\")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f\"Invalid beta-2 parameter: {betas[1]}\")\n        if not 0.0 <= beta_sigma < 1.0:\n            raise ValueError(f\"Invalid beta_sigma parameter: {beta_sigma}\")\n        if not 0.0 <= weight_decay:\n            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n        if not 0.0 <= schedule_decay:\n            raise ValueError(f\"Invalid schedule_decay value: {schedule_decay}\")\n        if not 0 <= warmup_steps:\n            raise ValueError(f\"Invalid warmup_steps value: {warmup_steps}\")\n\n        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay,\n                        eps=eps, beta_sigma=beta_sigma,\n                        schedule_decay=schedule_decay, warmup_steps=warmup_steps)\n        super().__init__(params, defaults)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            lr = group['lr']\n            wd = group['weight_decay']\n            eps = group['eps']\n            beta_sigma = group['beta_sigma']\n            schedule_decay = group['schedule_decay']\n            warmup_steps = group['warmup_steps']\n            beta1, beta2 = group['betas']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                g = p.grad\n                state = self.state[p]\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['m_schedule'] = 1.0\n                    state['m'] = torch.zeros_like(p)  # 1st moment\n                    state['v'] = torch.zeros_like(p)  # 2nd moment\n                    state['sigma'] = 0.6  # Scalar sigma per parameter\n                    state['g_prev'] = torch.zeros_like(p)  # Previous gradient\n\n                state['step'] += 1\n                step = state['step']\n                m, v, g_prev = state['m'], state['v'], state['g_prev']\n                m_schedule = state['m_schedule']\n                sigma = state['sigma']\n\n                # ===== Nadam Momentum Schedule (Optimized) =====\n                momentum_t = beta1 * (1. - 0.5 * (0.98 ** (step * schedule_decay)))\n                momentum_t1 = beta1 * (1. - 0.5 * (0.98 ** ((step + 1) * schedule_decay)))\n                m_schedule = m_schedule * momentum_t\n                schedule_next = m_schedule * momentum_t1\n\n                # ===== Efficient Weight Decay =====\n                if wd != 0:\n                    p.mul_(1 - lr * wd)\n\n                # ===== Low-Cost Moment Updates =====\n                # Update momentum (m)\n                m.mul_(beta1).add_(g, alpha=1 - beta1)\n\n                # Update adaptive term (v)\n                v.mul_(beta2).addcmul_(g, g, value=1 - beta2)\n\n                # ===== Ultra-Efficient Sigma Update =====\n                grad_change = (g - g_prev).abs().mean().item()\n                stability = 1.0 / (1.0 + 10 * grad_change)\n                new_sigma = 0.4 + 0.4 * stability\n\n                # Warmup-controlled sigma update\n                warmup_factor = min(1.0, step / warmup_steps)\n                sigma = sigma * (beta_sigma * warmup_factor) + new_sigma * (1 - beta_sigma * warmup_factor)\n                sigma = max(0.4, min(0.8, sigma))\n                state['sigma'] = sigma\n\n                # Store current gradient for next step\n                g_prev.copy_(g)\n\n                # ===== Fast Parameter Update =====\n                # Bias correction with momentum combination\n                bias_corr1 = 1 - beta1 ** step\n                bias_corr2 = 1 - beta2 ** step\n\n                m_hat = (beta1 * m) / (1 - beta1**(step+1)) + ((1 - beta1) * g) / (1 - beta1**step)\n                v_hat = v / bias_corr2\n                denom = v_hat.sqrt().add_(eps)\n\n                # Learning rate with warmup and sigma scaling\n                lr_scale = min(1.0, step / warmup_steps)\n                step_size = lr * lr_scale * (1. - momentum_t) / (1. - m_schedule) * sigma\n\n                p.addcdiv_(m_hat, denom, value=-step_size)\n\n                # Update momentum schedule\n                state['m_schedule'] = schedule_next\n\n        return loss\n\n# ---------------------------\n# Enhanced Metrics Tracking\n# ---------------------------\nclass MetricsTracker:\n    \"\"\"Enhanced metrics tracking with statistical analysis\"\"\"\n    def __init__(self):\n        self.metrics = {}\n        self.training_history = []\n    \n    def update_epoch(self, run_name: str, epoch: int, metrics: Dict[str, float]):\n        if run_name not in self.metrics:\n            self.metrics[run_name] = []\n        \n        epoch_data = {'epoch': epoch, **metrics}\n        self.metrics[run_name].append(epoch_data)\n        self.training_history.append({'run': run_name, **epoch_data})\n    \n    def get_summary_dataframe(self) -> pd.DataFrame:\n        \"\"\"Convert metrics to pandas DataFrame for analysis\"\"\"\n        df = pd.DataFrame(self.training_history)\n        return df\n    \n    def compute_statistical_significance(self, config1: str, config2: str, metric: str = 'test_acc') -> float:\n        \"\"\"Compute if differences are statistically significant\"\"\"\n        try:\n            import scipy.stats as stats\n            \n            if config1 not in self.metrics or config2 not in self.metrics:\n                return 1.0\n            \n            metrics1 = [m[metric] for m in self.metrics[config1] if metric in m]\n            metrics2 = [m[metric] for m in self.metrics[config2] if metric in m]\n            \n            if len(metrics1) < 2 or len(metrics2) < 2:\n                return 1.0\n            \n            t_stat, p_value = stats.ttest_ind(metrics1, metrics2)\n            return p_value\n        except ImportError:\n            return 1.0\n\n# ---------------------------\n# MNIST Data Loading\n# ---------------------------\ndef get_mnist_loaders(batch_size=128, data_root=\"./data\"):\n    cores = os.cpu_count() or 2\n    num_workers = min(8, max(2, cores // 2))\n\n    # MNIST specific transformations\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(28, padding=4),  # MNIST is 28x28\n        transforms.RandomHorizontalFlip(),  # Can still be useful for digits\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n    ])\n    \n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    # Load MNIST dataset\n    train_full = datasets.MNIST(root=data_root, train=True, download=True, transform=transform_train)\n    testset = datasets.MNIST(root=data_root, train=False, download=True, transform=transform_test)\n\n    # Split training set into train and validation\n    trainset, valset = random_split(train_full, [55000, 5000])  # MNIST has 60k training samples\n\n    loader_kwargs = dict(num_workers=num_workers, pin_memory=True, persistent_workers=True if num_workers > 0 else False)\n    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, **loader_kwargs)\n    val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, **loader_kwargs)\n    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, **loader_kwargs)\n    \n    print(f\"MNIST Dataset: {len(trainset)} training, {len(valset)} validation, {len(testset)} test samples\")\n    return train_loader, val_loader, test_loader\n\n# ---------------------------\n# MNIST Model Definition\n# ---------------------------\nclass MNIST_CNN(nn.Module):\n    \"\"\"A simple CNN model for MNIST classification\"\"\"\n    def __init__(self):\n        super(MNIST_CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # After two pools: 28->14->7\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout1(x)\n        x = x.view(-1, 64 * 7 * 7)\n        x = F.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\ndef get_mnist_model():\n    \"\"\"Returns a CNN model for MNIST\"\"\"\n    return MNIST_CNN()\n\n# ---------------------------\n# Training and Evaluation Loop\n# ---------------------------\ndef train_model(model_fn, optimizer_factory, train_loader, val_loader, test_loader,\n                num_epochs=10, run_name=\"default\", metrics_tracker=None):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model_fn()\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optimizer_factory(model.parameters())\n\n    scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n    autocast_ctx = lambda: torch.amp.autocast(device_type=device.type, enabled=USE_AMP)\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n    metrics = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n    overall_start_time = time.time()\n\n    print(f\"Starting training for '{run_name}' on {device}...\")\n\n    for epoch in range(num_epochs):\n        epoch_start = time.time()\n        model.train()\n        train_loss, train_correct, train_total = 0.0, 0, 0\n        for data, targets in train_loader:\n            data, targets = data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n\n            optimizer.zero_grad(set_to_none=True)\n            with autocast_ctx():\n                outputs = model(data)\n                loss = criterion(outputs, targets)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item() * data.size(0)\n            _, predicted = outputs.max(1)\n            train_total += targets.size(0)\n            train_correct += predicted.eq(targets).sum().item()\n\n        metrics['train_loss'].append(train_loss / train_total)\n        metrics['train_acc'].append(train_correct / train_total)\n\n        # Validation\n        model.eval()\n        val_loss, val_correct, val_total = 0.0, 0, 0\n        with torch.no_grad():\n            for data, targets in val_loader:\n                data, targets = data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n                with autocast_ctx():\n                    outputs = model(data)\n                    loss = criterion(outputs, targets)\n                val_loss += loss.item() * data.size(0)\n                _, predicted = outputs.max(1)\n                val_total += targets.size(0)\n                val_correct += predicted.eq(targets).sum().item()\n\n        metrics['val_loss'].append(val_loss / val_total)\n        metrics['val_acc'].append(val_correct / val_total)\n\n        # Update metrics tracker\n        if metrics_tracker is not None:\n            epoch_metrics = {\n                'train_loss': metrics['train_loss'][-1],\n                'val_loss': metrics['val_loss'][-1],\n                'train_acc': metrics['train_acc'][-1],\n                'val_acc': metrics['val_acc'][-1]\n            }\n            metrics_tracker.update_epoch(run_name, epoch, epoch_metrics)\n\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n              f\"Train Loss: {metrics['train_loss'][-1]:.4f}, Acc: {metrics['train_acc'][-1]:.4f} | \"\n              f\"Val Loss: {metrics['val_loss'][-1]:.4f}, Acc: {metrics['val_acc'][-1]:.4f} | \"\n              f\"Time: {time.time() - epoch_start:.2f}s\")\n\n    # Final Test Evaluation\n    model.eval()\n    test_loss, test_correct, test_total = 0.0, 0, 0\n    all_probs, all_targets = [], []\n    with torch.no_grad():\n        for data, targets in test_loader:\n            data, targets = data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n            outputs = model(data)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item() * data.size(0)\n            _, predicted = outputs.max(1)\n            test_total += targets.size(0)\n            test_correct += predicted.eq(targets).sum().item()\n            all_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    final_metrics = {\n        'test_acc': test_correct / test_total,\n        'test_loss': test_loss / test_total,\n        'avg_epoch_time': (time.time() - overall_start_time) / num_epochs\n    }\n\n    preds = np.argmax(all_probs, axis=1)\n    final_metrics['test_precision'] = precision_score(all_targets, preds, average='macro', zero_division=0)\n    final_metrics['test_recall'] = recall_score(all_targets, preds, average='macro', zero_division=0)\n    final_metrics['test_f1'] = f1_score(all_targets, preds, average='macro', zero_division=0)\n    \n    try:\n        final_metrics['test_auc'] = roc_auc_score(all_targets, all_probs, multi_class='ovr')\n    except:\n        final_metrics['test_auc'] = 0.0\n\n    print(f\"Test Results ({run_name}): Acc: {final_metrics['test_acc']:.4f}, F1: {final_metrics['test_f1']:.4f}\")\n    return {**metrics, **final_metrics}\n\n# ---------------------------\n# Ablation Study Setup for Griffin (12 Configurations)\n# ---------------------------\ndef get_ablated_hyperparams_griffin():\n    \"\"\"Focused hyperparameter search for Griffin optimizer (12 configurations)\"\"\"\n    default_cfg = {\n        'lr': 5e-4,\n        'betas': (0.95, 0.99),\n        'weight_decay': 1e-4,\n        'eps': 1e-8,\n        'beta_sigma': 0.9,\n        'schedule_decay': 5e-3,\n        'warmup_steps': 100\n    }\n    hyperparams = {}\n\n    # 1. Baseline\n    hyperparams[\"Default\"] = default_cfg.copy()\n    \n    # 2-3. Learning rate variations (most critical parameter)\n    for lr in [1e-4, 1e-3]:\n        cfg = default_cfg.copy()\n        cfg['lr'] = lr\n        hyperparams[f\"lr={lr}\"] = cfg\n\n    # 4-5. Beta variations (momentum and variance)\n    for betas in [(0.9, 0.999), (0.98, 0.999)]:\n        cfg = default_cfg.copy()\n        cfg['betas'] = betas\n        hyperparams[f\"Œ≤‚ÇÅ={betas[0]},Œ≤‚ÇÇ={betas[1]}\"] = cfg\n\n    # 6. Beta_sigma variation (sigma momentum)\n    cfg = default_cfg.copy()\n    cfg['beta_sigma'] = 0.95\n    hyperparams[\"Œ≤_œÉ=0.95\"] = cfg\n\n    # 7-8. Weight decay variations (regularization)\n    for wd in [0.0, 1e-3]:\n        cfg = default_cfg.copy()\n        cfg['weight_decay'] = wd\n        hyperparams[f\"wd={wd}\"] = cfg\n\n    # 9-10. Schedule decay variations (momentum scheduling)\n    for schedule_decay in [1e-3, 1e-2]:\n        cfg = default_cfg.copy()\n        cfg['schedule_decay'] = schedule_decay\n        hyperparams[f\"schedule_decay={schedule_decay}\"] = cfg\n\n    # 11. Warmup steps variations\n    cfg = default_cfg.copy()\n    cfg['warmup_steps'] = 500\n    hyperparams[\"warmup=500\"] = cfg\n\n    # 12. Combined aggressive configuration\n    cfg = default_cfg.copy()\n    cfg['lr'] = 1e-3\n    cfg['betas'] = (0.98, 0.999)\n    cfg['schedule_decay'] = 1e-2\n    hyperparams[\"HighLR+HighBeta+HighDecay\"] = cfg\n\n    return hyperparams\n\ndef validate_griffin_params(params):\n    \"\"\"Validate Griffin optimizer parameters\"\"\"\n    if not isinstance(params, dict):\n        raise TypeError(\"params must be a dictionary\")\n    \n    required_keys = ['lr', 'betas', 'weight_decay', 'eps', 'beta_sigma', \n                    'schedule_decay', 'warmup_steps']\n    \n    for key in required_keys:\n        if key not in params:\n            raise ValueError(f\"Missing required parameter: {key}\")\n    \n    beta1, beta2 = params['betas']\n    if not (0.0 <= beta1 < 1.0 and 0.0 <= beta2 < 1.0):\n        raise ValueError(f\"Invalid beta values: {params['betas']}\")\n    \n    if not 0.0 <= params['beta_sigma'] < 1.0:\n        raise ValueError(f\"Invalid beta_sigma: {params['beta_sigma']}\")\n    \n    if not 0.0 <= params['schedule_decay']:\n        raise ValueError(f\"Invalid schedule_decay: {params['schedule_decay']}\")\n    \n    if not 0 <= params['warmup_steps']:\n        raise ValueError(f\"Invalid warmup_steps: {params['warmup_steps']}\")\n    \n    return True\n\n# ---------------------------\n# Enhanced Visualization\n# ---------------------------\ndef create_griffin_comprehensive_plots(all_metrics: Dict[str, Dict], output_dir: str = \"./griffin_mnist_results\"):\n    \"\"\"Create comprehensive visualization of Griffin ablation results\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Performance comparison\n    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n    \n    # Accuracy and F1 comparison\n    configs = list(all_metrics.keys())\n    test_accs = [all_metrics[c]['test_acc'] for c in configs]\n    test_f1s = [all_metrics[c]['test_f1'] for c in configs]\n    \n    x_pos = np.arange(len(configs))\n    bars1 = ax1.bar(x_pos - 0.2, test_accs, 0.4, label='Accuracy', alpha=0.8, color='steelblue')\n    bars2 = ax1.bar(x_pos + 0.2, test_f1s, 0.4, label='F1-Score', alpha=0.8, color='darkorange')\n    \n    ax1.set_xlabel('Configurations')\n    ax1.set_ylabel('Scores')\n    ax1.set_title('Griffin on MNIST: Test Accuracy and F1-Score (12 Configurations)')\n    ax1.set_xticks(x_pos)\n    ax1.set_xticklabels(configs, rotation=45, ha='right')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Add value labels on bars\n    for bars in [bars1, bars2]:\n        for bar in bars:\n            height = bar.get_height()\n            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                    f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n    \n    # Training curves for top configurations\n    top_configs = sorted(configs, key=lambda x: all_metrics[x]['test_acc'], reverse=True)[:3]\n    for config in top_configs:\n        ax2.plot(all_metrics[config]['train_acc'], label=f'{config} (Train)', linewidth=2)\n        ax2.plot(all_metrics[config]['val_acc'], '--', label=f'{config} (Val)', linewidth=2)\n    \n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.set_title('Griffin on MNIST: Training Curves - Top 3 Configurations')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    # Convergence speed\n    convergence_data = []\n    for config in configs:\n        val_accs = all_metrics[config]['val_acc']\n        if len(val_accs) > 0:\n            max_acc = max(val_accs)\n            target_acc = 0.8 * max_acc\n            convergence_epoch = next((i for i, acc in enumerate(val_accs) if acc >= target_acc), len(val_accs))\n            convergence_data.append(convergence_epoch)\n        else:\n            convergence_data.append(len(val_accs))\n    \n    ax3.bar(configs, convergence_data, alpha=0.7, color='mediumpurple')\n    ax3.set_xlabel('Configurations')\n    ax3.set_ylabel('Epoch to Reach 80% Max Accuracy')\n    ax3.set_title('Griffin on MNIST: Convergence Speed')\n    ax3.set_xticklabels(configs, rotation=45, ha='right')\n    ax3.grid(True, alpha=0.3)\n    \n    # Performance vs computational efficiency\n    times = [all_metrics[c]['avg_epoch_time'] for c in configs]\n    accuracies = test_accs\n    \n    scatter = ax4.scatter(times, accuracies, s=100, alpha=0.7, c=test_f1s, cmap='coolwarm')\n    ax4.set_xlabel('Average Epoch Time (s)')\n    ax4.set_ylabel('Test Accuracy')\n    ax4.set_title('Griffin on MNIST: Accuracy vs Computational Efficiency')\n    ax4.grid(True, alpha=0.3)\n    \n    # Add colorbar for F1 scores\n    plt.colorbar(scatter, ax=ax4, label='F1-Score')\n    \n    # Annotate points\n    for i, config in enumerate(configs):\n        ax4.annotate(config, (times[i], accuracies[i]), \n                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n    \n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/griffin_mnist_ablation_12_configs.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\n    # Additional plot: Griffin-specific parameter analysis\n    fig, ax = plt.subplots(figsize=(14, 8))\n    \n    # Group by parameter type for sensitivity analysis\n    param_groups = {\n        'Learning Rate': [k for k in configs if k.startswith('lr=')],\n        'Beta Parameters': [k for k in configs if k.startswith('Œ≤')],\n        'Weight Decay': [k for k in configs if k.startswith('wd=')],\n        'Schedule Decay': [k for k in configs if k.startswith('schedule_decay=')],\n        'Beta Sigma': [k for k in configs if 'Œ≤_œÉ' in k],\n        'Warmup/Combined': [k for k in configs if 'warmup' in k or 'HighLR' in k]\n    }\n    \n    colors = plt.cm.viridis(np.linspace(0, 1, len(param_groups)))\n    \n    for i, (group_name, group_configs) in enumerate(param_groups.items()):\n        group_accs = [all_metrics[c]['test_acc'] for c in group_configs if c in all_metrics]\n        if group_accs:\n            mean_acc = np.mean(group_accs)\n            std_acc = np.std(group_accs)\n            ax.bar(group_name, mean_acc, color=colors[i], alpha=0.7, \n                  yerr=std_acc, capsize=5, label=f'{group_name} (n={len(group_accs)})')\n    \n    ax.set_ylabel('Test Accuracy')\n    ax.set_title('Griffin: Parameter Group Sensitivity Analysis on MNIST')\n    ax.grid(True, alpha=0.3)\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(f'{output_dir}/griffin_parameter_sensitivity.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\n# ---------------------------\n# Main Ablation Study Execution\n# ---------------------------\ndef run_griffin_ablation_study(num_epochs=10, batch_size=256):\n    \"\"\"Run Griffin ablation study with 12 configurations on MNIST\"\"\"\n    train_loader, val_loader, test_loader = get_mnist_loaders(batch_size=batch_size)\n    hyperparams = get_ablated_hyperparams_griffin()\n    all_metrics = {}\n    \n    # Initialize metrics tracker\n    metrics_tracker = MetricsTracker()\n    \n    print(f\"üöÄ Starting Griffin Ablation Study on MNIST with {len(hyperparams)} configurations\")\n    print(f\"üìä Epochs: {num_epochs}, Batch Size: {batch_size}\")\n    print(\"=\" * 100)\n    \n    for name, cfg in tqdm(hyperparams.items(), desc=\"Griffin Configurations\"):\n        print(f\"\\n‚ñ∂Ô∏è  Running: {name}\")\n        print(f\"   ‚öôÔ∏è  Config: {cfg}\")\n        \n        try:\n            validate_griffin_params(cfg)\n            \n            optimizer_factory = lambda params: Griffin(params, **cfg)\n            metrics = train_model(\n                get_mnist_model, optimizer_factory,\n                train_loader, val_loader, test_loader,\n                num_epochs=num_epochs, run_name=name,\n                metrics_tracker=metrics_tracker\n            )\n            all_metrics[name] = metrics\n                \n        except Exception as e:\n            print(f\"‚ùå Error in configuration {name}: {e}\")\n            continue\n\n    # Generate comprehensive analysis\n    print(\"\\n\" + \"üìà\" * 20 + \" GRIFFIN MNIST ANALYSIS \" + \"üìà\" * 20)\n    generate_detailed_griffin_analysis(all_metrics, metrics_tracker)\n    \n    return all_metrics, metrics_tracker\n\ndef generate_detailed_griffin_analysis(all_metrics, metrics_tracker):\n    \"\"\"Generate detailed analysis of Griffin results on MNIST\"\"\"\n    # Create comprehensive plots\n    create_griffin_comprehensive_plots(all_metrics)\n    \n    # Export to CSV for further analysis\n    df = metrics_tracker.get_summary_dataframe()\n    df.to_csv('griffin_mnist_ablation_12_configs.csv', index=False)\n    \n    # Save configurations\n    save_griffin_configs({k: v for k, v in get_ablated_hyperparams_griffin().items() \n                       if k in all_metrics})\n    \n    # Print results table\n    print(\"\\n\" + \"#\" * 80)\n    print(\" \" * 20 + \"Griffin Ablation Study Results on MNIST (12 Configurations)\")\n    print(\"#\" * 80)\n\n    best_config_name = max(all_metrics.keys(), key=lambda k: all_metrics[k]['test_acc'])\n    best_metrics = all_metrics[best_config_name]\n    \n    print(f\"\\nüèÜ Best Configuration: '{best_config_name}'\")\n    print(f\"   - Accuracy:  {best_metrics['test_acc']:.4f}\")\n    print(f\"   - F1-Score:  {best_metrics['test_f1']:.4f}\")\n    print(f\"   - AUC:       {best_metrics['test_auc']:.4f}\")\n    print(f\"   - Precision: {best_metrics['test_precision']:.4f}\")\n    print(f\"   - Time/Epoch: {best_metrics['avg_epoch_time']:.2f}s\")\n\n    print(\"\\n\" + \"-\" * 70)\n    print(\"LaTeX Table Summary:\")\n    print(\"-\" * 70)\n    print(r\"\\begin{tabular}{lcccccc}\")\n    print(r\"\\toprule\")\n    print(r\"\\textbf{Configuration} & \\textbf{Accuracy} & \\textbf{F1-Score} & \\textbf{AUC} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{Time/Epoch} \\\\\")\n    print(r\"\\midrule\")\n\n    sorted_names = sorted(all_metrics.keys(), key=lambda k: all_metrics[k]['test_acc'], reverse=True)\n    for name in sorted_names:\n        metrics = all_metrics[name]\n        is_best = name == best_config_name\n        acc_str = r\"\\textbf{\" + f\"{metrics['test_acc']:.4f}\" + \"}\" if is_best else f\"{metrics['test_acc']:.4f}\"\n        \n        # LaTeX-safe name\n        latex_name = name.replace('_', ' ').replace('Œ≤', r'$\\beta$').replace('œÉ', r'$\\sigma$')\n        \n        print(\n            latex_name + \" & \" +\n            acc_str + \" & \" +\n            f\"{metrics['test_f1']:.4f} & \" +\n            f\"{metrics['test_auc']:.4f} & \" +\n            f\"{metrics['test_precision']:.4f} & \" +\n            f\"{metrics['test_recall']:.4f} & \" +\n            f\"{metrics['avg_epoch_time']:.2f}s\" + r\" \\\\\"\n        )\n\n    print(r\"\\bottomrule\")\n    print(r\"\\end{tabular}\")\n\n    # Statistical significance analysis\n    configs = list(all_metrics.keys())\n    if len(configs) >= 2:\n        best_config = max(configs, key=lambda x: all_metrics[x]['test_acc'])\n        second_best = max([c for c in configs if c != best_config], \n                         key=lambda x: all_metrics[x]['test_acc'])\n        \n        p_value = metrics_tracker.compute_statistical_significance(\n            best_config, second_best\n        )\n        \n        print(f\"\\nüìä Statistical Significance Analysis:\")\n        print(f\"   Best vs Second Best: p-value = {p_value:.4f}\")\n        if p_value < 0.05:\n            print(\"   ‚úÖ Difference is statistically significant (p < 0.05)\")\n        else:\n            print(\"   ‚ö†Ô∏è  Difference is not statistically significant (p ‚â• 0.05)\")\n\n    # Griffin-specific parameter importance analysis\n    print(f\"\\nüîç Griffin Parameter Importance Summary:\")\n    param_groups = {\n        'Learning Rate': [k for k in configs if k.startswith('lr=')],\n        'Beta Parameters': [k for k in configs if k.startswith('Œ≤')],\n        'Weight Decay': [k for k in configs if k.startswith('wd=')],\n        'Schedule Decay': [k for k in configs if k.startswith('schedule_decay=')],\n        'Beta Sigma': [k for k in configs if 'Œ≤_œÉ' in k],\n        'Warmup Steps': [k for k in configs if 'warmup' in k],\n        'Combined Configs': [k for k in configs if 'HighLR' in k]\n    }\n    \n    for group_name, group_configs in param_groups.items():\n        if group_configs:\n            group_accs = [all_metrics[c]['test_acc'] for c in group_configs if c in all_metrics]\n            if group_accs:\n                mean_acc = np.mean(group_accs)\n                std_acc = np.std(group_accs)\n                print(f\"   {group_name:20}: {mean_acc:.4f} ¬± {std_acc:.4f} (n={len(group_accs)})\")\n\n    # Griffin-specific insights\n    print(f\"\\nüéØ Griffin-Specific Insights:\")\n    print(f\"   - Best accuracy: {best_metrics['test_acc']:.4f}\")\n    print(f\"   - Average accuracy across all configs: {np.mean([all_metrics[c]['test_acc'] for c in configs]):.4f}\")\n    print(f\"   - Standard deviation: {np.std([all_metrics[c]['test_acc'] for c in configs]):.4f}\")\n    \n    # Analyze sigma-related configurations\n    sigma_configs = [c for c in configs if 'Œ≤_œÉ' in c]\n    if sigma_configs:\n        sigma_accs = [all_metrics[c]['test_acc'] for c in sigma_configs if c in all_metrics]\n        print(f\"   - Beta_sigma configurations average: {np.mean(sigma_accs):.4f}\")\n    \n    # Analyze schedule decay effects\n    decay_configs = [c for c in configs if 'schedule_decay' in c]\n    if decay_configs:\n        decay_accs = [all_metrics[c]['test_acc'] for c in decay_configs if c in all_metrics]\n        print(f\"   - Schedule decay configurations average: {np.mean(decay_accs):.4f}\")\n    \n    # Analyze warmup effects\n    warmup_configs = [c for c in configs if 'warmup' in c]\n    if warmup_configs:\n        warmup_accs = [all_metrics[c]['test_acc'] for c in warmup_configs if c in all_metrics]\n        print(f\"   - Warmup configurations average: {np.mean(warmup_accs):.4f}\")\n\n    # Griffin algorithm specific observations\n    print(f\"\\n‚ö° Griffin Algorithm Observations:\")\n    print(f\"   - Uses Nadam-style momentum scheduling with exponential decay\")\n    print(f\"   - Features adaptive sigma based on gradient stability\")\n    print(f\"   - Implements warmup-controlled learning rate scaling\")\n    print(f\"   - Combines bias-corrected moments with sigma scaling\")\n\ndef save_griffin_configs(hyperparams, filename=\"griffin_mnist_ablation_12_configs.json\"):\n    \"\"\"Save Griffin ablation configurations for reproducibility\"\"\"\n    serializable_configs = {}\n    for name, cfg in hyperparams.items():\n        serializable_configs[name] = {\n            'lr': cfg['lr'],\n            'betas': list(cfg['betas']),\n            'weight_decay': cfg['weight_decay'],\n            'eps': cfg['eps'],\n            'beta_sigma': cfg['beta_sigma'],\n            'schedule_decay': cfg['schedule_decay'],\n            'warmup_steps': cfg['warmup_steps']\n        }\n    \n    with open(filename, 'w') as f:\n        json.dump(serializable_configs, f, indent=2)\n\nif __name__ == \"__main__\":\n    all_metrics, metrics_tracker = run_griffin_ablation_study(num_epochs=10, batch_size=256)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:01:57.029051Z","iopub.execute_input":"2025-11-17T21:01:57.029366Z","iopub.status.idle":"2025-11-17T21:23:14.627905Z","shell.execute_reply.started":"2025-11-17T21:01:57.029337Z","shell.execute_reply":"2025-11-17T21:23:14.627081Z"}},"outputs":[{"name":"stdout","text":"MNIST Dataset: 55000 training, 5000 validation, 10000 test samples\nüöÄ Starting Griffin Ablation Study on MNIST with 12 configurations\nüìä Epochs: 10, Batch Size: 256\n====================================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:   0%|          | 0/12 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n‚ñ∂Ô∏è  Running: Default\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.95, 0.99), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.005, 'warmup_steps': 100}\nStarting training for 'Default' on cuda...\nEpoch 1/10 | Train Loss: 1.7227, Acc: 0.3916 | Val Loss: 0.9342, Acc: 0.7182 | Time: 10.56s\nEpoch 2/10 | Train Loss: 0.8635, Acc: 0.7085 | Val Loss: 0.5524, Acc: 0.8362 | Time: 10.33s\nEpoch 3/10 | Train Loss: 0.6463, Acc: 0.7830 | Val Loss: 0.4308, Acc: 0.8678 | Time: 10.25s\nEpoch 4/10 | Train Loss: 0.5516, Acc: 0.8159 | Val Loss: 0.3666, Acc: 0.8854 | Time: 10.26s\nEpoch 5/10 | Train Loss: 0.5012, Acc: 0.8329 | Val Loss: 0.3422, Acc: 0.8910 | Time: 10.24s\nEpoch 6/10 | Train Loss: 0.4641, Acc: 0.8471 | Val Loss: 0.3153, Acc: 0.9000 | Time: 10.46s\nEpoch 7/10 | Train Loss: 0.4410, Acc: 0.8537 | Val Loss: 0.3042, Acc: 0.9024 | Time: 10.36s\nEpoch 8/10 | Train Loss: 0.4270, Acc: 0.8599 | Val Loss: 0.2926, Acc: 0.9074 | Time: 10.20s\nEpoch 9/10 | Train Loss: 0.4223, Acc: 0.8614 | Val Loss: 0.2916, Acc: 0.9056 | Time: 10.31s\nEpoch 10/10 | Train Loss: 0.4245, Acc: 0.8612 | Val Loss: 0.2922, Acc: 0.9066 | Time: 10.30s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:   8%|‚ñä         | 1/12 [01:44<19:09, 104.49s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (Default): Acc: 0.9353, F1: 0.9339\n\n‚ñ∂Ô∏è  Running: lr=0.0001\n   ‚öôÔ∏è  Config: {'lr': 0.0001, 'betas': (0.95, 0.99), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.005, 'warmup_steps': 100}\nStarting training for 'lr=0.0001' on cuda...\nEpoch 1/10 | Train Loss: 2.1620, Acc: 0.2264 | Val Loss: 1.7992, Acc: 0.4420 | Time: 10.37s\nEpoch 2/10 | Train Loss: 1.6614, Acc: 0.4245 | Val Loss: 1.3902, Acc: 0.5418 | Time: 10.35s\nEpoch 3/10 | Train Loss: 1.3911, Acc: 0.5223 | Val Loss: 1.1520, Acc: 0.6366 | Time: 10.35s\nEpoch 4/10 | Train Loss: 1.2017, Acc: 0.5902 | Val Loss: 0.9838, Acc: 0.7104 | Time: 10.30s\nEpoch 5/10 | Train Loss: 1.0845, Acc: 0.6331 | Val Loss: 0.8677, Acc: 0.7426 | Time: 10.33s\nEpoch 6/10 | Train Loss: 1.0037, Acc: 0.6624 | Val Loss: 0.8148, Acc: 0.7642 | Time: 10.45s\nEpoch 7/10 | Train Loss: 0.9544, Acc: 0.6790 | Val Loss: 0.7619, Acc: 0.7752 | Time: 10.34s\nEpoch 8/10 | Train Loss: 0.9346, Acc: 0.6844 | Val Loss: 0.7616, Acc: 0.7774 | Time: 10.21s\nEpoch 9/10 | Train Loss: 0.9133, Acc: 0.6935 | Val Loss: 0.7498, Acc: 0.7764 | Time: 10.15s\nEpoch 10/10 | Train Loss: 0.9071, Acc: 0.6963 | Val Loss: 0.7387, Acc: 0.7838 | Time: 10.32s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  17%|‚ñà‚ñã        | 2/12 [03:28<17:23, 104.38s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (lr=0.0001): Acc: 0.8554, F1: 0.8499\n\n‚ñ∂Ô∏è  Running: lr=0.001\n   ‚öôÔ∏è  Config: {'lr': 0.001, 'betas': (0.95, 0.99), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.005, 'warmup_steps': 100}\nStarting training for 'lr=0.001' on cuda...\nEpoch 1/10 | Train Loss: 1.4642, Acc: 0.4851 | Val Loss: 0.6273, Acc: 0.8024 | Time: 10.23s\nEpoch 2/10 | Train Loss: 0.6420, Acc: 0.7829 | Val Loss: 0.3957, Acc: 0.8750 | Time: 10.35s\nEpoch 3/10 | Train Loss: 0.4910, Acc: 0.8363 | Val Loss: 0.3291, Acc: 0.8910 | Time: 10.28s\nEpoch 4/10 | Train Loss: 0.4261, Acc: 0.8601 | Val Loss: 0.2653, Acc: 0.9212 | Time: 10.36s\nEpoch 5/10 | Train Loss: 0.3761, Acc: 0.8786 | Val Loss: 0.2336, Acc: 0.9262 | Time: 10.25s\nEpoch 6/10 | Train Loss: 0.3479, Acc: 0.8893 | Val Loss: 0.2128, Acc: 0.9358 | Time: 10.35s\nEpoch 7/10 | Train Loss: 0.3251, Acc: 0.8979 | Val Loss: 0.2045, Acc: 0.9356 | Time: 10.21s\nEpoch 8/10 | Train Loss: 0.3122, Acc: 0.9001 | Val Loss: 0.2055, Acc: 0.9384 | Time: 10.31s\nEpoch 9/10 | Train Loss: 0.3051, Acc: 0.9049 | Val Loss: 0.1944, Acc: 0.9404 | Time: 10.25s\nEpoch 10/10 | Train Loss: 0.3022, Acc: 0.9065 | Val Loss: 0.1903, Acc: 0.9448 | Time: 10.31s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  25%|‚ñà‚ñà‚ñå       | 3/12 [05:12<15:37, 104.21s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (lr=0.001): Acc: 0.9603, F1: 0.9596\n\n‚ñ∂Ô∏è  Running: Œ≤‚ÇÅ=0.9,Œ≤‚ÇÇ=0.999\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.9, 0.999), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.005, 'warmup_steps': 100}\nStarting training for 'Œ≤‚ÇÅ=0.9,Œ≤‚ÇÇ=0.999' on cuda...\nEpoch 1/10 | Train Loss: 1.5620, Acc: 0.4468 | Val Loss: 0.7079, Acc: 0.7896 | Time: 10.38s\nEpoch 2/10 | Train Loss: 0.7159, Acc: 0.7636 | Val Loss: 0.4617, Acc: 0.8490 | Time: 10.47s\nEpoch 3/10 | Train Loss: 0.5651, Acc: 0.8147 | Val Loss: 0.3714, Acc: 0.8810 | Time: 10.21s\nEpoch 4/10 | Train Loss: 0.4939, Acc: 0.8363 | Val Loss: 0.3354, Acc: 0.8918 | Time: 10.70s\nEpoch 5/10 | Train Loss: 0.4529, Acc: 0.8528 | Val Loss: 0.3051, Acc: 0.9060 | Time: 10.63s\nEpoch 6/10 | Train Loss: 0.4228, Acc: 0.8616 | Val Loss: 0.2886, Acc: 0.9078 | Time: 10.65s\nEpoch 7/10 | Train Loss: 0.4047, Acc: 0.8691 | Val Loss: 0.2686, Acc: 0.9188 | Time: 10.39s\nEpoch 8/10 | Train Loss: 0.3917, Acc: 0.8735 | Val Loss: 0.2675, Acc: 0.9226 | Time: 10.48s\nEpoch 9/10 | Train Loss: 0.3879, Acc: 0.8737 | Val Loss: 0.2642, Acc: 0.9172 | Time: 10.44s\nEpoch 10/10 | Train Loss: 0.3850, Acc: 0.8773 | Val Loss: 0.2622, Acc: 0.9192 | Time: 10.39s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  33%|‚ñà‚ñà‚ñà‚ñé      | 4/12 [06:58<13:59, 104.90s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (Œ≤‚ÇÅ=0.9,Œ≤‚ÇÇ=0.999): Acc: 0.9422, F1: 0.9411\n\n‚ñ∂Ô∏è  Running: Œ≤‚ÇÅ=0.98,Œ≤‚ÇÇ=0.999\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.98, 0.999), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.005, 'warmup_steps': 100}\nStarting training for 'Œ≤‚ÇÅ=0.98,Œ≤‚ÇÇ=0.999' on cuda...\nEpoch 1/10 | Train Loss: 1.7904, Acc: 0.3627 | Val Loss: 1.0239, Acc: 0.6748 | Time: 10.44s\nEpoch 2/10 | Train Loss: 0.9145, Acc: 0.6881 | Val Loss: 0.6099, Acc: 0.8080 | Time: 10.49s\nEpoch 3/10 | Train Loss: 0.6770, Acc: 0.7750 | Val Loss: 0.4529, Acc: 0.8622 | Time: 10.35s\nEpoch 4/10 | Train Loss: 0.5658, Acc: 0.8129 | Val Loss: 0.3832, Acc: 0.8758 | Time: 10.61s\nEpoch 5/10 | Train Loss: 0.5068, Acc: 0.8344 | Val Loss: 0.3525, Acc: 0.8850 | Time: 10.74s\nEpoch 6/10 | Train Loss: 0.4747, Acc: 0.8444 | Val Loss: 0.3346, Acc: 0.8986 | Time: 10.63s\nEpoch 7/10 | Train Loss: 0.4568, Acc: 0.8517 | Val Loss: 0.3116, Acc: 0.9024 | Time: 10.62s\nEpoch 8/10 | Train Loss: 0.4426, Acc: 0.8561 | Val Loss: 0.3065, Acc: 0.9080 | Time: 10.54s\nEpoch 9/10 | Train Loss: 0.4366, Acc: 0.8592 | Val Loss: 0.3010, Acc: 0.9052 | Time: 10.61s\nEpoch 10/10 | Train Loss: 0.4308, Acc: 0.8606 | Val Loss: 0.2961, Acc: 0.9054 | Time: 10.88s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 [08:45<12:19, 105.67s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (Œ≤‚ÇÅ=0.98,Œ≤‚ÇÇ=0.999): Acc: 0.9289, F1: 0.9272\n\n‚ñ∂Ô∏è  Running: Œ≤_œÉ=0.95\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.95, 0.99), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.95, 'schedule_decay': 0.005, 'warmup_steps': 100}\nStarting training for 'Œ≤_œÉ=0.95' on cuda...\nEpoch 1/10 | Train Loss: 1.7367, Acc: 0.3881 | Val Loss: 0.8898, Acc: 0.7416 | Time: 10.67s\nEpoch 2/10 | Train Loss: 0.8256, Acc: 0.7253 | Val Loss: 0.5111, Acc: 0.8416 | Time: 10.52s\nEpoch 3/10 | Train Loss: 0.6215, Acc: 0.7945 | Val Loss: 0.4148, Acc: 0.8678 | Time: 10.77s\nEpoch 4/10 | Train Loss: 0.5351, Acc: 0.8238 | Val Loss: 0.3606, Acc: 0.8880 | Time: 10.75s\nEpoch 5/10 | Train Loss: 0.4870, Acc: 0.8415 | Val Loss: 0.3216, Acc: 0.8980 | Time: 10.75s\nEpoch 6/10 | Train Loss: 0.4558, Acc: 0.8505 | Val Loss: 0.3059, Acc: 0.9090 | Time: 10.66s\nEpoch 7/10 | Train Loss: 0.4271, Acc: 0.8613 | Val Loss: 0.2878, Acc: 0.9094 | Time: 10.31s\nEpoch 8/10 | Train Loss: 0.4186, Acc: 0.8662 | Val Loss: 0.2730, Acc: 0.9144 | Time: 10.52s\nEpoch 9/10 | Train Loss: 0.4151, Acc: 0.8661 | Val Loss: 0.2749, Acc: 0.9158 | Time: 10.60s\nEpoch 10/10 | Train Loss: 0.4104, Acc: 0.8691 | Val Loss: 0.2726, Acc: 0.9144 | Time: 10.40s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 6/12 [10:32<10:36, 106.16s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (Œ≤_œÉ=0.95): Acc: 0.9372, F1: 0.9357\n\n‚ñ∂Ô∏è  Running: wd=0.0\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.95, 0.99), 'weight_decay': 0.0, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.005, 'warmup_steps': 100}\nStarting training for 'wd=0.0' on cuda...\nEpoch 1/10 | Train Loss: 1.7577, Acc: 0.3758 | Val Loss: 0.9853, Acc: 0.7026 | Time: 10.26s\nEpoch 2/10 | Train Loss: 0.8810, Acc: 0.7008 | Val Loss: 0.5665, Acc: 0.8208 | Time: 10.42s\nEpoch 3/10 | Train Loss: 0.6524, Acc: 0.7825 | Val Loss: 0.4472, Acc: 0.8572 | Time: 10.36s\nEpoch 4/10 | Train Loss: 0.5634, Acc: 0.8120 | Val Loss: 0.3830, Acc: 0.8790 | Time: 11.08s\nEpoch 5/10 | Train Loss: 0.5089, Acc: 0.8311 | Val Loss: 0.3489, Acc: 0.8914 | Time: 10.72s\nEpoch 6/10 | Train Loss: 0.4734, Acc: 0.8440 | Val Loss: 0.3193, Acc: 0.8978 | Time: 10.73s\nEpoch 7/10 | Train Loss: 0.4508, Acc: 0.8518 | Val Loss: 0.3037, Acc: 0.9042 | Time: 10.43s\nEpoch 8/10 | Train Loss: 0.4397, Acc: 0.8583 | Val Loss: 0.3007, Acc: 0.9052 | Time: 10.56s\nEpoch 9/10 | Train Loss: 0.4303, Acc: 0.8594 | Val Loss: 0.2990, Acc: 0.9010 | Time: 10.55s\nEpoch 10/10 | Train Loss: 0.4218, Acc: 0.8619 | Val Loss: 0.2872, Acc: 0.9080 | Time: 10.56s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7/12 [12:19<08:51, 106.37s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (wd=0.0): Acc: 0.9286, F1: 0.9267\n\n‚ñ∂Ô∏è  Running: wd=0.001\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.95, 0.99), 'weight_decay': 0.001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.005, 'warmup_steps': 100}\nStarting training for 'wd=0.001' on cuda...\nEpoch 1/10 | Train Loss: 1.7253, Acc: 0.3914 | Val Loss: 0.8861, Acc: 0.7396 | Time: 10.64s\nEpoch 2/10 | Train Loss: 0.8282, Acc: 0.7240 | Val Loss: 0.5071, Acc: 0.8462 | Time: 10.57s\nEpoch 3/10 | Train Loss: 0.6120, Acc: 0.7980 | Val Loss: 0.4028, Acc: 0.8750 | Time: 11.11s\nEpoch 4/10 | Train Loss: 0.5229, Acc: 0.8271 | Val Loss: 0.3438, Acc: 0.8954 | Time: 10.70s\nEpoch 5/10 | Train Loss: 0.4716, Acc: 0.8434 | Val Loss: 0.3134, Acc: 0.8976 | Time: 10.77s\nEpoch 6/10 | Train Loss: 0.4381, Acc: 0.8582 | Val Loss: 0.2892, Acc: 0.9086 | Time: 10.60s\nEpoch 7/10 | Train Loss: 0.4213, Acc: 0.8633 | Val Loss: 0.2806, Acc: 0.9106 | Time: 10.59s\nEpoch 8/10 | Train Loss: 0.4012, Acc: 0.8711 | Val Loss: 0.2714, Acc: 0.9170 | Time: 10.71s\nEpoch 9/10 | Train Loss: 0.3942, Acc: 0.8719 | Val Loss: 0.2573, Acc: 0.9200 | Time: 10.80s\nEpoch 10/10 | Train Loss: 0.3941, Acc: 0.8743 | Val Loss: 0.2623, Acc: 0.9178 | Time: 10.69s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 [14:08<07:08, 107.01s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (wd=0.001): Acc: 0.9391, F1: 0.9377\n\n‚ñ∂Ô∏è  Running: schedule_decay=0.001\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.95, 0.99), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.001, 'warmup_steps': 100}\nStarting training for 'schedule_decay=0.001' on cuda...\nEpoch 1/10 | Train Loss: 1.6851, Acc: 0.4019 | Val Loss: 0.8609, Acc: 0.7238 | Time: 10.68s\nEpoch 2/10 | Train Loss: 0.8253, Acc: 0.7209 | Val Loss: 0.5276, Acc: 0.8316 | Time: 10.90s\nEpoch 3/10 | Train Loss: 0.6166, Acc: 0.7938 | Val Loss: 0.4050, Acc: 0.8748 | Time: 10.43s\nEpoch 4/10 | Train Loss: 0.5201, Acc: 0.8258 | Val Loss: 0.3534, Acc: 0.8884 | Time: 10.52s\nEpoch 5/10 | Train Loss: 0.4730, Acc: 0.8449 | Val Loss: 0.3102, Acc: 0.8970 | Time: 10.49s\nEpoch 6/10 | Train Loss: 0.4397, Acc: 0.8561 | Val Loss: 0.2846, Acc: 0.9070 | Time: 10.44s\nEpoch 7/10 | Train Loss: 0.4209, Acc: 0.8630 | Val Loss: 0.2746, Acc: 0.9134 | Time: 10.37s\nEpoch 8/10 | Train Loss: 0.4014, Acc: 0.8695 | Val Loss: 0.2653, Acc: 0.9140 | Time: 10.55s\nEpoch 9/10 | Train Loss: 0.3952, Acc: 0.8720 | Val Loss: 0.2667, Acc: 0.9148 | Time: 10.47s\nEpoch 10/10 | Train Loss: 0.3925, Acc: 0.8747 | Val Loss: 0.2594, Acc: 0.9178 | Time: 10.48s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 9/12 [15:54<05:20, 106.84s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (schedule_decay=0.001): Acc: 0.9400, F1: 0.9386\n\n‚ñ∂Ô∏è  Running: schedule_decay=0.01\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.95, 0.99), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.01, 'warmup_steps': 100}\nStarting training for 'schedule_decay=0.01' on cuda...\nEpoch 1/10 | Train Loss: 1.6930, Acc: 0.4025 | Val Loss: 0.8515, Acc: 0.7440 | Time: 10.57s\nEpoch 2/10 | Train Loss: 0.8234, Acc: 0.7263 | Val Loss: 0.5176, Acc: 0.8416 | Time: 10.45s\nEpoch 3/10 | Train Loss: 0.6316, Acc: 0.7912 | Val Loss: 0.4202, Acc: 0.8644 | Time: 10.40s\nEpoch 4/10 | Train Loss: 0.5403, Acc: 0.8203 | Val Loss: 0.3671, Acc: 0.8796 | Time: 10.55s\nEpoch 5/10 | Train Loss: 0.4880, Acc: 0.8397 | Val Loss: 0.3205, Acc: 0.8994 | Time: 10.45s\nEpoch 6/10 | Train Loss: 0.4595, Acc: 0.8485 | Val Loss: 0.3077, Acc: 0.9000 | Time: 10.42s\nEpoch 7/10 | Train Loss: 0.4354, Acc: 0.8598 | Val Loss: 0.2882, Acc: 0.9088 | Time: 10.39s\nEpoch 8/10 | Train Loss: 0.4240, Acc: 0.8621 | Val Loss: 0.2886, Acc: 0.9072 | Time: 10.56s\nEpoch 9/10 | Train Loss: 0.4196, Acc: 0.8641 | Val Loss: 0.2837, Acc: 0.9108 | Time: 10.47s\nEpoch 10/10 | Train Loss: 0.4167, Acc: 0.8637 | Val Loss: 0.2774, Acc: 0.9136 | Time: 10.63s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 [17:40<03:33, 106.59s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (schedule_decay=0.01): Acc: 0.9354, F1: 0.9339\n\n‚ñ∂Ô∏è  Running: warmup=500\n   ‚öôÔ∏è  Config: {'lr': 0.0005, 'betas': (0.95, 0.99), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.005, 'warmup_steps': 500}\nStarting training for 'warmup=500' on cuda...\nEpoch 1/10 | Train Loss: 2.1129, Acc: 0.2385 | Val Loss: 1.5869, Acc: 0.4916 | Time: 10.45s\nEpoch 2/10 | Train Loss: 1.3174, Acc: 0.5460 | Val Loss: 0.8012, Acc: 0.7644 | Time: 10.82s\nEpoch 3/10 | Train Loss: 0.7991, Acc: 0.7317 | Val Loss: 0.5089, Acc: 0.8428 | Time: 10.73s\nEpoch 4/10 | Train Loss: 0.6227, Acc: 0.7903 | Val Loss: 0.4178, Acc: 0.8698 | Time: 10.91s\nEpoch 5/10 | Train Loss: 0.5522, Acc: 0.8169 | Val Loss: 0.3768, Acc: 0.8820 | Time: 10.66s\nEpoch 6/10 | Train Loss: 0.5023, Acc: 0.8337 | Val Loss: 0.3521, Acc: 0.8890 | Time: 10.49s\nEpoch 7/10 | Train Loss: 0.4757, Acc: 0.8457 | Val Loss: 0.3312, Acc: 0.8952 | Time: 10.42s\nEpoch 8/10 | Train Loss: 0.4581, Acc: 0.8499 | Val Loss: 0.3200, Acc: 0.9038 | Time: 10.47s\nEpoch 9/10 | Train Loss: 0.4528, Acc: 0.8538 | Val Loss: 0.3169, Acc: 0.8990 | Time: 10.53s\nEpoch 10/10 | Train Loss: 0.4469, Acc: 0.8545 | Val Loss: 0.3155, Acc: 0.8990 | Time: 10.46s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 11/12 [19:27<01:46, 106.74s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (warmup=500): Acc: 0.9263, F1: 0.9244\n\n‚ñ∂Ô∏è  Running: HighLR+HighBeta+HighDecay\n   ‚öôÔ∏è  Config: {'lr': 0.001, 'betas': (0.98, 0.999), 'weight_decay': 0.0001, 'eps': 1e-08, 'beta_sigma': 0.9, 'schedule_decay': 0.01, 'warmup_steps': 100}\nStarting training for 'HighLR+HighBeta+HighDecay' on cuda...\nEpoch 1/10 | Train Loss: 1.5806, Acc: 0.4371 | Val Loss: 0.6907, Acc: 0.7910 | Time: 10.58s\nEpoch 2/10 | Train Loss: 0.6843, Acc: 0.7721 | Val Loss: 0.4061, Acc: 0.8670 | Time: 10.49s\nEpoch 3/10 | Train Loss: 0.5154, Acc: 0.8292 | Val Loss: 0.3379, Acc: 0.8912 | Time: 10.57s\nEpoch 4/10 | Train Loss: 0.4471, Acc: 0.8521 | Val Loss: 0.2849, Acc: 0.9082 | Time: 10.52s\nEpoch 5/10 | Train Loss: 0.4055, Acc: 0.8671 | Val Loss: 0.2582, Acc: 0.9162 | Time: 10.45s\nEpoch 6/10 | Train Loss: 0.3820, Acc: 0.8762 | Val Loss: 0.2377, Acc: 0.9254 | Time: 10.56s\nEpoch 7/10 | Train Loss: 0.3641, Acc: 0.8825 | Val Loss: 0.2231, Acc: 0.9276 | Time: 10.49s\nEpoch 8/10 | Train Loss: 0.3544, Acc: 0.8861 | Val Loss: 0.2227, Acc: 0.9300 | Time: 10.46s\nEpoch 9/10 | Train Loss: 0.3473, Acc: 0.8885 | Val Loss: 0.2114, Acc: 0.9314 | Time: 10.52s\nEpoch 10/10 | Train Loss: 0.3450, Acc: 0.8896 | Val Loss: 0.2111, Acc: 0.9334 | Time: 10.41s\n","output_type":"stream"},{"name":"stderr","text":"Griffin Configurations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [21:13<00:00, 106.16s/it]","output_type":"stream"},{"name":"stdout","text":"Test Results (HighLR+HighBeta+HighDecay): Acc: 0.9528, F1: 0.9518\n\nüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìà GRIFFIN MNIST ANALYSIS üìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìàüìà\n","output_type":"stream"},{"name":"stderr","text":"\n/tmp/ipykernel_48/3190714570.py:530: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax3.set_xticklabels(configs, rotation=45, ha='right')\n","output_type":"stream"},{"name":"stdout","text":"\n################################################################################\n                    Griffin Ablation Study Results on MNIST (12 Configurations)\n################################################################################\n\nüèÜ Best Configuration: 'lr=0.001'\n   - Accuracy:  0.9603\n   - F1-Score:  0.9596\n   - AUC:       0.9989\n   - Precision: 0.9597\n   - Time/Epoch: 10.39s\n\n----------------------------------------------------------------------\nLaTeX Table Summary:\n----------------------------------------------------------------------\n\\begin{tabular}{lcccccc}\n\\toprule\n\\textbf{Configuration} & \\textbf{Accuracy} & \\textbf{F1-Score} & \\textbf{AUC} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{Time/Epoch} \\\\\n\\midrule\nlr=0.001 & \\textbf{0.9603} & 0.9596 & 0.9989 & 0.9597 & 0.9596 & 10.39s \\\\\nHighLR+HighBeta+HighDecay & 0.9528 & 0.9518 & 0.9984 & 0.9521 & 0.9516 & 10.62s \\\\\n$\\beta$‚ÇÅ=0.9,$\\beta$‚ÇÇ=0.999 & 0.9422 & 0.9411 & 0.9975 & 0.9414 & 0.9409 & 10.59s \\\\\nschedule decay=0.001 & 0.9400 & 0.9386 & 0.9971 & 0.9393 & 0.9383 & 10.64s \\\\\nwd=0.001 & 0.9391 & 0.9377 & 0.9971 & 0.9380 & 0.9376 & 10.83s \\\\\n$\\beta$ $\\sigma$=0.95 & 0.9372 & 0.9357 & 0.9969 & 0.9359 & 0.9358 & 10.70s \\\\\nschedule decay=0.01 & 0.9354 & 0.9339 & 0.9970 & 0.9348 & 0.9337 & 10.60s \\\\\nDefault & 0.9353 & 0.9339 & 0.9966 & 0.9343 & 0.9338 & 10.44s \\\\\n$\\beta$‚ÇÅ=0.98,$\\beta$‚ÇÇ=0.999 & 0.9289 & 0.9272 & 0.9962 & 0.9279 & 0.9271 & 10.70s \\\\\nwd=0.0 & 0.9286 & 0.9267 & 0.9960 & 0.9281 & 0.9263 & 10.68s \\\\\nwarmup=500 & 0.9263 & 0.9244 & 0.9956 & 0.9250 & 0.9242 & 10.70s \\\\\nlr=0.0001 & 0.8554 & 0.8499 & 0.9870 & 0.8618 & 0.8506 & 10.42s \\\\\n\\bottomrule\n\\end{tabular}\n\nüìä Statistical Significance Analysis:\n   Best vs Second Best: p-value = 1.0000\n   ‚ö†Ô∏è  Difference is not statistically significant (p ‚â• 0.05)\n\nüîç Griffin Parameter Importance Summary:\n   Learning Rate       : 0.9079 ¬± 0.0524 (n=2)\n   Beta Parameters     : 0.9361 ¬± 0.0055 (n=3)\n   Weight Decay        : 0.9339 ¬± 0.0053 (n=2)\n   Schedule Decay      : 0.9377 ¬± 0.0023 (n=2)\n   Beta Sigma          : 0.9372 ¬± 0.0000 (n=1)\n   Warmup Steps        : 0.9263 ¬± 0.0000 (n=1)\n   Combined Configs    : 0.9528 ¬± 0.0000 (n=1)\n\nüéØ Griffin-Specific Insights:\n   - Best accuracy: 0.9603\n   - Average accuracy across all configs: 0.9318\n   - Standard deviation: 0.0249\n   - Beta_sigma configurations average: 0.9372\n   - Schedule decay configurations average: 0.9377\n   - Warmup configurations average: 0.9263\n\n‚ö° Griffin Algorithm Observations:\n   - Uses Nadam-style momentum scheduling with exponential decay\n   - Features adaptive sigma based on gradient stability\n   - Implements warmup-controlled learning rate scaling\n   - Combines bias-corrected moments with sigma scaling\n","output_type":"stream"}],"execution_count":3}]}